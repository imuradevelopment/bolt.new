# ===== BE .env.example =====

# ---------- Server ----------
PORT=4000
CORS_ORIGIN=http://localhost:3000

# ---------- Database (Postgres) ----------
DB_USER=postgres
DB_PASSWORD=postgres
DB_NAME=llm_platform
POSTGRES_URL=postgres://${DB_USER}:${DB_PASSWORD}@db:5432/${DB_NAME}

# ---------- Auth ----------
JWT_SECRET=change-me-to-a-long-random-string

# ---------- LLM (Default provider) ----------
# FEから選択可能だが、既定はここで指定（gemini | azure-openai）
LLM_PROVIDER=gemini

# ---------- Gemini ----------
GEMINI_API_KEY=    # 伏せる（ここにキーを設定）
# 'models/' 接頭辞は不要でも可（内部で正規化）
GEMINI_MODEL=gemini-2.5-pro

# ---------- Azure OpenAI ----------
AZURE_OPENAI_API_KEY=          # 伏せる（ここにキーを設定）
# リソースURLでもフルURLでも可（下はフルURL例：deployment と api-version を自動抽出）
AZURE_OPENAI_ENDPOINT=https://<resource>.openai.azure.com/openai/deployments/<deployment>/chat/completions?api-version=2025-01-01-preview
# 明示したい場合のみ（上記URLに無ければ使用）
# AZURE_OPENAI_API_VERSION=2025-01-01-preview
# AZURE_OPENAI_DEPLOYMENT=<deployment-name>

# ---------- Generation defaults (暫定) ----------
# 将来はHubで自動計算（現状は安全側の既定値）
LLM_MAX_TOKENS=4096
LLM_MAX_RESPONSE_SEGMENTS=0
LLM_TEMPERATURE=1
LLM_TOP_P=0.95
# LLM_TOP_K=40  # （Geminiのみ必要時）
# 例: 出力比や上限を将来使う場合の指針（今は未使用）
# LLM_OUTPUT_TOKENS_RATIO=0.25
# LLM_OUTPUT_TOKENS_ABS_MAX=8192

# ---------- pgAdmin ----------
PGADMIN_DEFAULT_EMAIL=admin@example.com